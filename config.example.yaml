bot_type:
  method: BOTH  # "DISCORD", "WEB",or "BOTH". If "WEB" or "BOTH", ensure to set a valid port below.

# HTTP server auth (optional, required if bot_type is WEB or BOTH)
http:
  html_port: 8000
  html_host: 0.0.0.0  # Bind host for the web server (default 127.0.0.1; set 0.0.0.0 to expose on LAN)
  bearer_token: ""  # When set, required on /chat (and may be applied to /reset). Leave blank to disable.
  message_limit: 200  # message to store per room, older messages are discarded to keep under this limit. (default: 200)
  inactive_room_days: 7 # number of days after which inactive rooms are cleared. (default: 1)


LOG_LEVEL: INFO # INFO, DEBUG, FULL - INFO: standard levels; DEBUG adds more detail; FULL includes full request/response bodies (no secrets) 
LIB_LOG_LEVEL: WARNING  # Controls verbosity of third-party libs (discord/httpx/httpcore). Typical: WARNING or ERROR; set DEBUG for deep library tracing.
LOG_PROMPTS: false   # if true, prompt and response text are saved as files in logs/prompts-YYYYMMDD-HHMMSS for debugging; may include sensitive info so use with caution
LOG_CONSOLE: false  # if true, console logged to logs/log.log for debugging
LOG_ERRORS: true   # if true, ERROR level logs logged to logs/errors.log for debugging

# Openrouter configuration - get API key from https://openrouter.ai/ add into .env
model:
  # Provider selection order per context (names: openrouter, openai) (openai = any OpenAI-compatible local backend, ie kobold, llama.cpp, llmstudio etc)
  provider_order:
    normal: [openrouter, openai]
    nsfw: [openai, openrouter]
    vision: [openrouter] # list multimodal models under vision lower in config, only openrouter currently supported
  temperature: 0.6  # temperature controls randomness of output (0.0 = deterministic, 1.0 = very random)
  top_p: 1.0
  max_tokens: 4096  # max_tokens controls maximum completion tokens (reply size)
  context_window: 20480 # context_window: total context window (prompt + completion) supported by the selected model
  concurrency: 3  # number of concurrent requests to the LLM API
  retry_attempts: 2 # number of retry attempts for failed requests (0 = no retries)
  openrouter:    #openrouter settings
    models: # uses first model then sequential order if until one is available, https://openrouter.ai/models for more.
    #  - openai/gpt-5-mini
      - x-ai/grok-4-fast
      - x-ai/grok-4
      - openai/gpt-5-chat
      - openai/gpt-5
      - google/gemini-2.0-flash-exp:free
      - meta-llama/llama-4-scout:free
      - meta-llama/llama-4-scout
      - tngtech/deepseek-r1t2-chimera:free
    allow_auto_fallback: false # use openrouter's automatic fallback if the main model is down, this may incur cost
    # base_url: https://openrouter.ai/api/v1/chat/completions  # override if needed
    http_referer: https://yourwebsite.com # optional for listing in Openrouter leaderboards/dashboards
    x_title: Your Fancy Name  # optional for listing in Openrouter leaderboards/dashboards
    vision:
      enabled: true
      max_images: 4
      models:
      #  - openai/gpt-5-mini
        - x-ai/grok-4
        - openai/gpt-5-chat
        - openai/gpt-5
        - x-ai/grok-4-fast
        - google/gemini-2.0-flash-exp:free
        - meta-llama/llama-4-scout:free
        - meta-llama/llama-4-scout
      mode: single-pass
      retry_on_image_count_error: true
      fallback_to_text: true
      apply_in:
        mentions: true
        replies: true
        general_chat: true
        batch: true
      log_image_urls: false
      timeout_multiplier: 1.5
    web:
      enabled: true
      models:
        - openai/gpt-4o-mini-search-preview
        - openai/gpt-4.1-mini
        - openai/gpt-4.1
        - perplexity/sonar
      engine: native  # native (provider only), exa (exa only )or undefined (native with exa fallback)
      max_results: 3  # openrouter default is 5
      search_context_size: low  # low, medium, high
  openai:
    enabled: true
    base_url: http://127.0.0.1:5001/v1/
    concurrency: 1
    timeout: 600.0
    retry_attempts: 1
  stop:
    - <|eot|> # End of text token for LLaMA models
    - <|im_end|> # End of message token for ChatML models
    - <|end▁of▁sentence|> # End of message token for DeepSeek models
    - <end_of_turn> # End of message for Gemma models

rate_limits: # Anti-spam settings, hard cap per channel to avoid flooding LLM calls
  window_seconds: 300 # time window in seconds
  max_responses: 10 # max responses allowed in the time window
  warning_ttl_seconds: 10 # duration of warning message in seconds

participation:  # if and how the bot participates in chats
  allow_nsfw: true # if true, bot uses NSFW system prompt in NSFW channels, otherwise will always use normal system prompt
  mention_required: false # if true, bot only responds when mentioned; if false, may respond in general chat
  respond_to_name: true # Respond to name aliases even if not mentioned
  random_response_chance: 0.2 # % chance (as decimal) for general chat responses
  general_chat: # settings for general chat participation
    allowed_channels: # list of Discord channel IDs where general chat responses are allowed; empty means none.
      - '123456789012345678'
      - '1234567890123456789'
      - '2345678901234567890'
      - '1266743874949353472'
    response_chance_override: # list of Discord channel IDs where random response chance is 100%, these id's must be in allowed_channels
      - '123456789012345678'
      - '1234567890123456789'

  bots:
    respond_to_bots: true # if true, bot may respond to other bot messages
    blocked_bot_ids: '' # comma-separated Discord user IDs bot should ignore even if respond_to_bots=true
  conversation_mode:
    enabled: true # if true, bot maintains conversation context in threads and DMs
    include_non_replies: true # if true, bot includes non-reply messages in conversation threads
    window_seconds: 75  # time window in seconds to continue conversation mode after last bot reply
    max_messages: 6 # max messages to include in conversation mode context
    batch_interval_seconds: 10  # time window in seconds to batch multiple messages into one response
    batch_limit: 10 # max messages to batch into one response
    affects_cooldown: false # if true, conversation mode responses count against participation.cooldown
  cooldown:
    logic_type: AND # "AND" or "OR" - if "AND", all conditions must be met to allow a response; if "OR", any single condition met allows a response
    min_messages_between_replies: 10  # for general chat, trigger allowed if >= this many messages since last reply
    min_seconds_between_replies: 900  # minutes; trigger allowed if >= this many seconds since last reply
  context_on_time_cooldown:
    minutes: 10 # if >0, include recent context messages within this many minutes even if not in conversation mode
    max_messages: 50  # max recent messages to include when context_on_time_cooldown is active

persona: default

context:
  window_size: 15 # max number of recent messages to consider for context
  use_template: true  # if true, use context template for formatting; else simple chat history
  keep_history_tail: 4  # always keep this many recent messages regardless of recency
  recency_minutes: 15 # if >0, only include messages within this many minutes
  cluster_max_messages: 10  # max messages to include when clustering by topic
  thread_affinity_max: 6  # max messages to include from the current thread
  context_template_path: prompts/context_template.txt # template for formatting context messages
  lore:
    enabled: true # if true, use lore files for additional context
    paths:  # Accepts one or more JSON files with SillyTavern-like schema under 'entries', all entries are 'on', matches only first keywords, constant = true always loaded. md markdown lore files are always loaded if present
    #  - lore/your_lore.md
    #  - lore/more_lore.json
    md_priority: low  # high = higher priority than JSON lore, low = lower priority than JSON lore, md files are always loaded if present
    max_fraction: 0.33  # max fraction of context to use for lore (0.0 to 1.0)

discord:
  intents:  # ensure to set the right flags in the Discord Developer Portal as well
    message_content: true # required to read message content
    members: false  # if true, bot can access guild member info (not usually needed)
    presences: false  # if true, bot can access user presence info (not usually needed)
    message_char_limit: 2000  # max characters of a message to read (Discord limit is currently 2000)
    max_response_messages: 3  # how many messages to split a long response into (1 = no splitting)
  admin_user_ids: # Discord user IDs to class as admin (ie can run / admin commands)
    - '123456789012345678'
  elevated_user_ids: # Discord user IDs to class as elevated (ie can run / elevated commands)
    - '123456789012345678'
    - '234567890123456789'
