bot_type:
  method: BOTH  # "DISCORD", "WEB",or "BOTH". If "WEB" or "BOTH", ensure to set a valid port below.

# HTTP server auth (optional, required if bot_type is WEB or BOTH)
http:
  html_port: 8005
  html_host: 0.0.0.0  # Bind host for the web server (default 127.0.0.1; set 0.0.0.0 to expose on LAN)
  bearer_token: "abc123"  # When set, required on /chat (and may be applied to /reset). Leave blank to disable.

LOG_LEVEL: INFO # INFO, DEBUG, FULL - INFO: standard levels; DEBUG adds more detail; FULL includes full request/response bodies (no secrets) 
LIB_LOG_LEVEL: WARNING  # Controls verbosity of third-party libs (discord/httpx/httpcore). Typical: WARNING or ERROR; set DEBUG for deep library tracing.
LOG_PROMPTS: false   # if true, prompt and response text are saved as files in logs/prompts-YYYYMMDD-HHMMSS for debugging; may include sensitive info so use with caution
LOG_CONSOLE: false  # if true, console logged to logs/log.log for debugging
LOG_ERRORS: true   # if true, ERROR level logs logged to logs/errors.log for debugging

# Openrouter configuration - get API key from https://openrouter.ai/ add into .env
model:
  # Provider selection order per context (names: openrouter, openai) (openai = any OpenAI-compatible local backend, ie kobold, llama.cpp, llmstudio etc)
  provider_order:
    normal: [openrouter, openai]
    nsfw: [openai, openrouter]
    vision: [openrouter] # list multimodal models under vision lower in config, only openrouter currently supported
  temperature: 0.6  # temperature controls randomness of output (0.0 = deterministic, 1.0 = very random)
  top_p: 1.0
  max_tokens: 4096  # max_tokens controls maximum completion tokens (reply size)
  context_window: 20480 # context_window: total context window (prompt + completion) supported by the selected model
  concurrency: 3  # number of concurrent requests to the LLM API
  retry_attempts: 2 # number of retry attempts for failed requests (0 = no retries)
  provider: openrouter    #openrouter settings
  models: # uses first model then sequential order if until one is available, https://openrouter.ai/models for more.
    - openai/gpt-5-mini
    - openai/gpt-5-chat
    - openai/gpt-5
    - x-ai/grok-4-fast
    - google/gemini-2.0-flash-exp:free
    - meta-llama/llama-4-scout:free
    - google/gemini-2.5-flash
    - meta-llama/llama-4-scout
  allow_auto_fallback: false # use openrouter's automatic fallback if the main model is down, this may incur cost
  # base_url: https://openrouter.ai/api/v1/chat/completions  # override if needed
  http_referer: https://yoursitehere.com # optional for listing in Openrouter leaderboards/dashboards
  x_title: Discord LLM Bot  # optional for listing in Openrouter leaderboards/dashboards
  vision:
    enabled: true
    max_images: 4
    models:
      - openai/gpt-5-mini
      - openai/gpt-5-chat
      - openai/gpt-5
      - x-ai/grok-4-fast
      - google/gemini-2.0-flash-exp:free
      - meta-llama/llama-4-scout:free
      - meta-llama/llama-4-scout
    mode: single-pass
    retry_on_image_count_error: true
    fallback_to_text: true
    apply_in:
      mentions: true
      replies: true
      general_chat: true
      batch: true
    log_image_urls: false
    timeout_multiplier: 1.5
  openai:
    enabled: true
    base_url: http://127.0.0.1:5001/v1/
    concurrency: 1
    timeout: 300.0
    retry_attempts: 1
  stop:
    - <|eot|> # End of text token for LLaMA models
    - <|im_end|> # End of message token for ChatML models
    - <|end▁of▁sentence|> # End of message token for DeepSeek models
    - <end_of_turn> # End of message for Gemma models

rate_limits: # Anti-spam settings, hard cap per channel to avoid flooding LLM calls
  window_seconds: 300 # time window in seconds
  max_responses: 10 # max responses allowed in the time window
  warning_ttl_seconds: 10 # duration of warning message in seconds

participation:  # if and how the bot participates in chats
  allow_nsfw: true # if true, bot uses NSFW system prompt in NSFW channels, otherwise will always use normal system prompt
  mention_required: false # if true, bot only responds when mentioned; if false, may respond in general chat
  respond_to_name: true # Respond to name aliases even if not mentioned
  random_response_chance: 0.2 # % chance (as decimal) for general chat responses
  general_chat: # settings for general chat participation
    allowed_channels: # list of Discord channel IDs where general chat responses are allowed; empty means none.
      - '1234567890123456789' # channel 1
      - '2345678901234567890' # channel 2
    response_chance_override: # list of Discord channel IDs where random response chance is 100%, these id's must be in allowed_channels
      - '1234567890123456789' # channel 1

  bots:
    respond_to_bots: true # if true, bot may respond to other bot messages
    blocked_bot_ids: '' # comma-separated Discord user IDs bot should ignore even if respond_to_bots=true
  conversation_mode:
    enabled: true # if true, bot maintains conversation context in threads and DMs
    include_non_replies: true # if true, bot includes non-reply messages in conversation threads
    window_seconds: 75  # time window in seconds to continue conversation mode after last bot reply
    max_messages: 6 # max messages to include in conversation mode context
    batch_interval_seconds: 10  # time window in seconds to batch multiple messages into one response
    batch_limit: 10 # max messages to batch into one response
    affects_cooldown: false # if true, conversation mode responses count against participation.cooldown
  cooldown:
    min_messages_between_replies: 10  # for general chat, trigger allowed if >= this many messages since last reply
    min_seconds_between_replies: 900  # minutes; trigger allowed if >= this many seconds since last reply
  context_on_time_cooldown:
    minutes: 10 # if >0, include recent context messages within this many minutes even if not in conversation mode
    max_messages: 50  # max recent messages to include when context_on_time_cooldown is active

persona: default

context:
  window_size: 15 # max number of recent messages to consider for context
  use_template: true  # if true, use context template for formatting; else simple chat history
  keep_history_tail: 4  # always keep this many recent messages regardless of recency
  recency_minutes: 15 # if >0, only include messages within this many minutes
  cluster_max_messages: 10  # max messages to include when clustering by topic
  thread_affinity_max: 6  # max messages to include from the current thread
  context_template_path: prompts/context_template.txt # template for formatting context messages
  lore:
    enabled: true # if true, use lore files for additional context
    paths:  # Additional lore to use (these supplement lore in persona), json or markdown format
    #  - lore/default.json
      #- lore/default.md
    md_priority: low  # high = higher priority than JSON lore, low = lower priority than JSON lore, md files are always loaded if present
    max_fraction: 0.33  # max fraction of context to use for lore (0.0 to 1.0)

discord:
  intents:  # ensure to set the right flags in the Discord Developer Portal as well
    message_content: true # required to read message content
    members: false  # if true, bot can access guild member info (not usually needed)
    presences: false  # if true, bot can access user presence info (not usually needed)
    message_char_limit: 2000  # max characters of a message to read (Discord limit is currently 2000)
    max_response_messages: 3  # how many messages to split a long response into (1 = no splitting)
  admin_user_ids: # Discord user IDs to class as admin (ie can run / admin commands)
    - '098765432109876543'
  elevated_user_ids: # Discord user IDs to class as elevated (ie can run / elevated commands)
    - '098765432109876543'
    - '987654321098765434'
